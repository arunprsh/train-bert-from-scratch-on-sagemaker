{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a363a0-de09-4fa5-8ce2-a01e3afe12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b1c3a12-6917-4591-bca5-acba8ff36565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158871c3-f17c-421e-a41c-b501a16dd1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24573f2a-e708-43b9-b117-9e1be0ef060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a06eef-0ffa-4c5c-aa78-908c852c9eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_tokenizer': <tokenizers.Tokenizer at 0x5569eeb19430>,\n",
       " '_parameters': {'model': 'BertWordPiece',\n",
       "  'unk_token': '[UNK]',\n",
       "  'sep_token': '[SEP]',\n",
       "  'cls_token': '[CLS]',\n",
       "  'pad_token': '[PAD]',\n",
       "  'mask_token': '[MASK]',\n",
       "  'clean_text': True,\n",
       "  'handle_chinese_chars': True,\n",
       "  'strip_accents': None,\n",
       "  'lowercase': True,\n",
       "  'wordpieces_prefix': '##'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e72410bf-4f28-4edb-a22d-b01a2fff2b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [str(x) for x in Path('./').glob('*.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f0417b2-de72-457d-8e70-6c171f4eb78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample-1.txt', 'sample-2.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "745dd89a-10f9-4cef-a9aa-8023fc861e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=paths, vocab_size=30522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01abafde-2f37-463b-a8f7-32297d7fb53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_tokenizer': <tokenizers.Tokenizer at 0x5569eec3f3b0>,\n",
       " '_parameters': {'model': 'BertWordPiece',\n",
       "  'unk_token': '[UNK]',\n",
       "  'sep_token': '[SEP]',\n",
       "  'cls_token': '[CLS]',\n",
       "  'pad_token': '[PAD]',\n",
       "  'mask_token': '[MASK]',\n",
       "  'clean_text': True,\n",
       "  'handle_chinese_chars': True,\n",
       "  'strip_accents': None,\n",
       "  'lowercase': True,\n",
       "  'wordpieces_prefix': '##'}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2cf2740-d65e-4073-80b8-615bbc08fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=33, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./vocab.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "tokenizer.save_model('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e1e41-3344-45f7-8a97-d9bb6cdb56c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c77d575-196e-43de-8d93-e61279ddc076",
   "metadata": {},
   "source": [
    "#### Load the trained tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9473b806-36ec-4698-81eb-853e37b8e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "tokenizer = BertWordPieceTokenizer('./vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f98a33ac-fa35-4920-8b24-1b70f5666b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'covid19', 'is', 'a', 'virus', '[SEP]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('covid19 is a virus').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ccddc91-3b95-46f7-aeba-f534151a4465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11467"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id('covid19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "265b5c25-770b-49e8-8d83-41566f29901a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_parameters',\n",
       " '_tokenizer',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'decoder',\n",
       " 'enable_padding',\n",
       " 'enable_truncation',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'from_file',\n",
       " 'get_vocab',\n",
       " 'get_vocab_size',\n",
       " 'id_to_token',\n",
       " 'model',\n",
       " 'no_padding',\n",
       " 'no_truncation',\n",
       " 'normalize',\n",
       " 'normalizer',\n",
       " 'num_special_tokens_to_add',\n",
       " 'padding',\n",
       " 'post_process',\n",
       " 'post_processor',\n",
       " 'pre_tokenizer',\n",
       " 'save',\n",
       " 'save_model',\n",
       " 'to_str',\n",
       " 'token_to_id',\n",
       " 'train',\n",
       " 'train_from_iterator',\n",
       " 'truncation']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8eaf192c-c6c0-45d6-8759-c71aec4b92b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694a637-70bb-4d1f-b609-4270ae521078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
