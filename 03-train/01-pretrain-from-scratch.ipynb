{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip install sagemaker==2.100.0\n",
    "!pip install jedi==0.17  # this is a requirement for pygmentize to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "import sagemaker\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log versions of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using SageMaker: 2.100.0]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using SageMaker: {sagemaker.__version__}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "ROLE = get_execution_role()\n",
    "S3_BUCKET = session.default_bucket()\n",
    "ENTRY_POINT = 'pretrain.py'\n",
    "SOURCE_DIR = './src'\n",
    "INSTANCE_TYPE = 'ml.p4d.24xlarge'\n",
    "INSTANCE_COUNT = 4\n",
    "EBS_VOLUME_SIZE = 1024\n",
    "TRANSFORMERS_VERSION = '4.17.0'\n",
    "PYTORCH_VERSION = '1.10.2'\n",
    "PYTHON_VERSION = 'py38'\n",
    "BASE_JOB_NAME = 'hf-sm-pretrain-scratch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S3 bucket = sagemaker-us-east-1-119174016168\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'S3 bucket = {S3_BUCKET}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataCollatorForLanguageModeling\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainingArguments\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertTokenizerFast\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertForMaskedLM\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36ms3\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m S3Downloader\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msession\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Session\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36ms3\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m S3Uploader\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pipeline \n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_dataset\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DatasetDict\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Trainer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# Setup logging\u001b[39;49;00m\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logging.basicConfig(level=logging.getLevelName(\u001b[33m'\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \n",
      "                    handlers=[logging.StreamHandler(sys.stdout)], \n",
      "                    \u001b[36mformat\u001b[39;49;00m=\u001b[33m'\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# Log versions of dependencies\u001b[39;49;00m\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Transformers: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtransformers.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using SageMaker: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msagemaker.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Datasets: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdatasets.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Torch: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtorch.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Boto3: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mboto3.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Pandas: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpd.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mParsing command line arguments\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--master_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSMDATAPARALLEL_SERVER_ADDR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[37m# [IMPORTANT] Hyperparameters sent by the client (Studio notebook with the driver code to launch training) \u001b[39;49;00m\n",
      "    \u001b[37m# are passed as command-line arguments to the training script\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--s3_bucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--chunk_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_train_epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--per_device_train_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--region\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    \n",
      "    args, _ = parser.parse_known_args()\n",
      "    current_host = args.current_host\n",
      "    master_host = args.master_host\n",
      "    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcurrent_host\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mMaster host = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmaster_host\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    S3_BUCKET = args.s3_bucket\n",
      "    MAX_LENGTH = args.max_len\n",
      "    CHUNK_SIZE = args.chunk_size\n",
      "    TRAIN_EPOCHS = args.num_train_epochs\n",
      "    BATCH_SIZE = args.per_device_train_batch_size\n",
      "    REGION = args.region \n",
      "    SAVE_STEPS = \u001b[34m10000\u001b[39;49;00m\n",
      "    SAVE_TOTAL_LIMIT = \u001b[34m2\u001b[39;49;00m\n",
      "    \n",
      "    LOCAL_DATA_DIR = \u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/data/processed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    LOCAL_MODEL_DIR = \u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/custom\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    \n",
      "    config = BertConfig()\n",
      "    \n",
      "    \u001b[37m# Setup SageMaker Session for S3Downloader and S3Uploader \u001b[39;49;00m\n",
      "    boto_session = boto3.session.Session(region_name=REGION)\n",
      "    sm_session = sagemaker.Session(boto_session=boto_session)\n",
      "    \n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mdownload\u001b[39;49;00m(s3_path: \u001b[36mstr\u001b[39;49;00m, ebs_path: \u001b[36mstr\u001b[39;49;00m, session: Session) -> \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[34mtry\u001b[39;49;00m:\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(ebs_path):\n",
      "                os.makedirs(ebs_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "            S3Downloader.download(s3_path, ebs_path, sagemaker_session=session)\n",
      "        \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileExistsError\u001b[39;49;00m:  \u001b[37m# to avoid race condition between GPUs\u001b[39;49;00m\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mIgnoring FileExistsError to avoid I/O race conditions.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mIgnoring FileNotFoundError to avoid I/O race conditions.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mupload\u001b[39;49;00m(ebs_path: \u001b[36mstr\u001b[39;49;00m, s3_path: \u001b[36mstr\u001b[39;49;00m, session: Session) -> \u001b[34mNone\u001b[39;49;00m:\n",
      "        S3Uploader.upload(ebs_path, s3_path, sagemaker_session=session)\n",
      "        \n",
      "    \n",
      "    \u001b[37m# Download saved custom vocabulary file from S3 to local input path of the training cluster\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mDownloading custom vocabulary from [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/data/vocab/] to [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    path = os.path.join(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mvocab\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    download(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/data/vocab/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, path, sm_session)\n",
      "         \n",
      "    \u001b[37m# Download preprocessed datasets from S3 to local EBS volume (cache dir)\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mDownloading preprocessed datasets from [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/data/processed/] to [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_DATA_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    download(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/data/processed/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_DATA_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, sm_session)\n",
      "    \n",
      "    \u001b[37m# Re-create BERT WordPiece tokenizer \u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mRe-creating BERT tokenizer using custom vocabulary from [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    tokenizer = BertTokenizerFast.from_pretrained(path, config=config)\n",
      "    tokenizer.model_max_length = MAX_LENGTH\n",
      "    tokenizer.init_kwargs[\u001b[33m'\u001b[39;49;00m\u001b[33mmodel_max_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = MAX_LENGTH\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTokenizer: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtokenizer\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Read dataset \u001b[39;49;00m\n",
      "    chunked_datasets = datasets.load_from_disk(LOCAL_DATA_DIR)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mChunked datasets: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mchunked_datasets\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Create data collator\u001b[39;49;00m\n",
      "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
      "                                                    mlm=\u001b[34mTrue\u001b[39;49;00m, \n",
      "                                                    mlm_probability=\u001b[34m0.15\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Load MLM\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mLoading BertForMaskedLM model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    mlm = BertForMaskedLM(config=config)\n",
      "    \n",
      "    \u001b[37m# Train MLM\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTraining MLM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    training_args = TrainingArguments(output_dir=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                      overwrite_output_dir=\u001b[34mTrue\u001b[39;49;00m, \n",
      "                                      optim=\u001b[33m'\u001b[39;49;00m\u001b[33madamw_torch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                      num_train_epochs=TRAIN_EPOCHS,\n",
      "                                      per_device_train_batch_size=BATCH_SIZE,\n",
      "                                      evaluation_strategy=\u001b[33m'\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                      save_steps=SAVE_STEPS, \n",
      "                                      save_total_limit=SAVE_TOTAL_LIMIT)\n",
      "    trainer = Trainer(model=mlm, \n",
      "                      args=training_args, \n",
      "                      data_collator=data_collator,\n",
      "                      train_dataset=chunked_datasets[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "                      eval_dataset=chunked_datasets[\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[37m# Evaluate trained model for perplexity\u001b[39;49;00m\n",
      "    eval_results = trainer.evaluate()\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mPerplexity before training: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmath.exp(eval_results[\u001b[33m'\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[33m:\u001b[39;49;00m\u001b[33m.2f\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    trainer.train()\n",
      "    \n",
      "    eval_results = trainer.evaluate()\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mPerplexity after training: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmath.exp(eval_results[\u001b[33m'\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[33m:\u001b[39;49;00m\u001b[33m.2f\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m current_host == master_host:\n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(LOCAL_MODEL_DIR):\n",
      "            os.makedirs(LOCAL_MODEL_DIR, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "            \n",
      "        \u001b[37m# Save trained model to local model directory\u001b[39;49;00m\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mSaving trained MLM to [/tmp/cache/model/custom/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        trainer.save_model(LOCAL_MODEL_DIR)\n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m os.path.exists(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/pytorch_model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m os.path.exists(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/config.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "            \u001b[37m# Copy trained model from local directory of the training cluster to S3 \u001b[39;49;00m\n",
      "            logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCopying saved model from local to [s3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/model/custom/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            upload(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/model/custom/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, sm_session)\n",
      "\n",
      "            \u001b[37m# Copy vocab.txt to local model directory - this is needed to re-create the trained MLM\u001b[39;49;00m\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mCopying custom vocabulary to local model artifacts location to faciliate model evaluation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            shutil.copyfile(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpath\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "            \u001b[37m# Copy vocab.txt to saved model artifacts location in S3\u001b[39;49;00m\n",
      "            logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCopying custom vocabulary from [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpath\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab.txt] to [s3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/model/custom/] for future stages of ML pipeline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            upload(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpath\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/model/custom/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, sm_session)\n",
      "\n",
      "            \u001b[37m# Evaluate trained model for fill mask task\u001b[39;49;00m\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mCreate fill-mask task pipeline to evaluate trained MLM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            fill_mask = pipeline(\u001b[33m'\u001b[39;49;00m\u001b[33mfill-mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, model=LOCAL_MODEL_DIR)\n",
      "            df = pd.read_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/data/eval/eval_mlm.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "            \u001b[34mfor\u001b[39;49;00m gt, masked_sentence \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(df.ground_truth.tolist(), df.masked.tolist()):\n",
      "                logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mGround Truth    : \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mgt\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mMasked sentence : \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmasked_sentence\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                predictions = fill_mask(masked_sentence, top_k=\u001b[34m10\u001b[39;49;00m)\n",
      "                \u001b[34mfor\u001b[39;49;00m i, prediction \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(predictions):\n",
      "                    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mRank: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mi+\u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m | \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m(prediction[\u001b[33m\"\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] * \u001b[34m100\u001b[39;49;00m)\u001b[33m:\u001b[39;49;00m\u001b[33m.2f\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m % | \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m[prediction[\u001b[33m\"\u001b[39;49;00m\u001b[33mtoken_str\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m * \u001b[34m10\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./src/pretrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the estimator \n",
    "\n",
    "##### > Documentation on SageMaker HuggingFace Estimator can be found [here](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {'train': f's3://{S3_BUCKET}/data'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512  # Context size for BERT tokenizer \n",
    "CHUNK_SIZE = 128  \n",
    "TRAIN_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "REGION = 'us-east-1'  # [IMPORTANT] Change this to the region you are running your training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {'s3_bucket': S3_BUCKET, \n",
    "                   'max_len': MAX_LENGTH,\n",
    "                   'chunk_size': CHUNK_SIZE,\n",
    "                   'num_train_epochs': TRAIN_EPOCHS, \n",
    "                   'per_device_train_batch_size': BATCH_SIZE, \n",
    "                   'region': REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTRIBUTION_STRATEGY = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point=ENTRY_POINT, \n",
    "                                    source_dir=SOURCE_DIR, \n",
    "                                    role=ROLE, \n",
    "                                    instance_type=INSTANCE_TYPE, \n",
    "                                    instance_count=INSTANCE_COUNT,\n",
    "                                    volume_size=EBS_VOLUME_SIZE,\n",
    "                                    hyperparameters=HYPERPARAMETERS,\n",
    "                                    distribution=DISTRIBUTION_STRATEGY,\n",
    "                                    transformers_version=TRANSFORMERS_VERSION, \n",
    "                                    pytorch_version=PYTORCH_VERSION, \n",
    "                                    py_version=PYTHON_VERSION, \n",
    "                                    disable_profiler=True,\n",
    "                                    debugger_hook_config=False, \n",
    "                                    base_job_name=BASE_JOB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating training-job with name: hf-sm-pretrain-scratch-2022-09-23-04-00-22-965\n",
      "train request: {\n",
      "    \"AlgorithmSpecification\": {\n",
      "        \"TrainingInputMode\": \"File\",\n",
      "        \"TrainingImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\",\n",
      "        \"EnableSageMakerMetricsTimeSeries\": true\n",
      "    },\n",
      "    \"OutputDataConfig\": {\n",
      "        \"S3OutputPath\": \"s3://sagemaker-us-east-1-119174016168/\"\n",
      "    },\n",
      "    \"TrainingJobName\": \"hf-sm-pretrain-scratch-2022-09-23-04-00-22-965\",\n",
      "    \"StoppingCondition\": {\n",
      "        \"MaxRuntimeInSeconds\": 86400\n",
      "    },\n",
      "    \"ResourceConfig\": {\n",
      "        \"VolumeSizeInGB\": 1024,\n",
      "        \"InstanceCount\": 4,\n",
      "        \"InstanceType\": \"ml.p4d.24xlarge\"\n",
      "    },\n",
      "    \"RoleArn\": \"arn:aws:iam::119174016168:role/service-role/AmazonSageMaker-ExecutionRole-20211014T093628\",\n",
      "    \"InputDataConfig\": [\n",
      "        {\n",
      "            \"DataSource\": {\n",
      "                \"S3DataSource\": {\n",
      "                    \"S3DataType\": \"S3Prefix\",\n",
      "                    \"S3Uri\": \"s3://sagemaker-us-east-1-119174016168/data\",\n",
      "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
      "                }\n",
      "            },\n",
      "            \"ChannelName\": \"train\"\n",
      "        }\n",
      "    ],\n",
      "    \"HyperParameters\": {\n",
      "        \"s3_bucket\": \"\\\"sagemaker-us-east-1-119174016168\\\"\",\n",
      "        \"max_len\": \"512\",\n",
      "        \"chunk_size\": \"128\",\n",
      "        \"num_train_epochs\": \"50\",\n",
      "        \"per_device_train_batch_size\": \"32\",\n",
      "        \"region\": \"\\\"us-east-1\\\"\",\n",
      "        \"sagemaker_submit_directory\": \"\\\"s3://sagemaker-us-east-1-119174016168/hf-sm-pretrain-scratch-2022-09-23-04-00-22-965/source/sourcedir.tar.gz\\\"\",\n",
      "        \"sagemaker_program\": \"\\\"pretrain.py\\\"\",\n",
      "        \"sagemaker_container_log_level\": \"20\",\n",
      "        \"sagemaker_job_name\": \"\\\"hf-sm-pretrain-scratch-2022-09-23-04-00-22-965\\\"\",\n",
      "        \"sagemaker_region\": \"\\\"us-east-1\\\"\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": \"true\",\n",
      "        \"sagemaker_instance_type\": \"\\\"ml.p4d.24xlarge\\\"\",\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\\\"\\\"\"\n",
      "    },\n",
      "    \"Environment\": {\n",
      "        \"USE_SMDEBUG\": \"0\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(DATA, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
