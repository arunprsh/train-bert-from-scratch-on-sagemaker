{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune for Multi-class classification using OOB BERT MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip install sagemaker==2.100.0\n",
    "!pip install jedi==0.17  # this is a requirement for pygmentize to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "import sagemaker\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log versions of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using SageMaker: 2.100.0]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using SageMaker: {sagemaker.__version__}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "ROLE = get_execution_role()\n",
    "S3_BUCKET = session.default_bucket()\n",
    "ENTRY_POINT = 'finetune_clf_oob.py'\n",
    "SOURCE_DIR = './src'\n",
    "INSTANCE_TYPE = 'ml.p3.16xlarge'\n",
    "INSTANCE_COUNT = 4\n",
    "EBS_VOLUME_SIZE = 1024\n",
    "TRANSFORMERS_VERSION = '4.17.0'\n",
    "PYTORCH_VERSION = '1.10.2'\n",
    "PYTHON_VERSION = 'py38'\n",
    "BASE_JOB_NAME = 'hf-sm-clf-oob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S3 bucket = sagemaker-us-east-1-119174016168\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'S3 bucket = {S3_BUCKET}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m precision_recall_fscore_support\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertForSequenceClassification\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainingArguments\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertTokenizerFast\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msession\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Session\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36ms3\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m S3Downloader\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36ms3\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m S3Uploader\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_dataset\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pipeline\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Trainer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DatasetDict\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m \n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# Setup logging\u001b[39;49;00m\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logging.basicConfig(level=logging.getLevelName(\u001b[33m'\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \n",
      "                    handlers=[logging.StreamHandler(sys.stdout)], \n",
      "                    \u001b[36mformat\u001b[39;49;00m=\u001b[33m'\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# Log versions of dependencies\u001b[39;49;00m\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Transformers: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtransformers.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using SageMaker: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msagemaker.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Datasets: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdatasets.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Sklearn: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msklearn.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Torch: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtorch.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Pandas: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpd.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mHandling command line arguments\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--master_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSMDATAPARALLEL_SERVER_ADDR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[37m# [IMPORTANT] Hyperparameters sent by the client (Studio notebook with the driver code to launch training) \u001b[39;49;00m\n",
      "    \u001b[37m# are passed as command-line arguments to the training script\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--s3_bucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_train_epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--per_device_train_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--region\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    \n",
      "    args, _ = parser.parse_known_args()\n",
      "    current_host = args.current_host\n",
      "    master_host = args.master_host\n",
      "    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcurrent_host\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mMaster host = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmaster_host\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    S3_BUCKET = args.s3_bucket\n",
      "    MAX_LENGTH = args.max_len\n",
      "    TRAIN_EPOCHS = args.num_train_epochs\n",
      "    BATCH_SIZE = args.per_device_train_batch_size\n",
      "    REGION = args.region\n",
      "    SAVE_STEPS = \u001b[34m10000\u001b[39;49;00m\n",
      "    SAVE_TOTAL_LIMIT = \u001b[34m2\u001b[39;49;00m\n",
      "    \n",
      "    LOCAL_DATA_DIR = \u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/data/bert/processed-clf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    LOCAL_MODEL_DIR = \u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/finetuned-clf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    LOCAL_LABEL_DIR = \u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# Setup SageMaker Session for S3Downloader and S3Uploader \u001b[39;49;00m\n",
      "    boto_session = boto3.session.Session(region_name=REGION)\n",
      "    sm_session = sagemaker.Session(boto_session=boto_session)\n",
      "    \n",
      "    \u001b[37m# Load BERT Sequence Model \u001b[39;49;00m\n",
      "    model = BertForSequenceClassification.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, num_labels=\u001b[34m5\u001b[39;49;00m,  force_download=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Re-create original BERT WordPiece tokenizer \u001b[39;49;00m\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mRe-creating original BERT tokenizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    tokenizer = BertTokenizerFast.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTokenizer: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtokenizer\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mdownload\u001b[39;49;00m(s3_path: \u001b[36mstr\u001b[39;49;00m, ebs_path: \u001b[36mstr\u001b[39;49;00m, session: Session) -> \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[34mtry\u001b[39;49;00m:\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(ebs_path):\n",
      "                os.makedirs(ebs_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "            S3Downloader.download(s3_path, ebs_path, sagemaker_session=session)\n",
      "        \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileExistsError\u001b[39;49;00m:  \u001b[37m# to avoid race condition between GPUs\u001b[39;49;00m\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mIgnoring FileExistsError to avoid I/O race conditions.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mIgnoring FileNotFoundError to avoid I/O race conditions.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mupload\u001b[39;49;00m(ebs_path: \u001b[36mstr\u001b[39;49;00m, s3_path: \u001b[36mstr\u001b[39;49;00m, session: Session) -> \u001b[34mNone\u001b[39;49;00m:\n",
      "        S3Uploader.upload(ebs_path, s3_path, sagemaker_session=session)\n",
      "    \n",
      "    \u001b[37m# Download preprocessed datasets from S3 to local EBS volume (cache dir)\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mDownloading preprocessed datasets from [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/data/processed/] to [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_DATA_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    download(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/data/bert/processed-clf/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_DATA_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, sm_session)\n",
      "    \n",
      "    \u001b[37m# Load tokenized dataset \u001b[39;49;00m\n",
      "    tokenized_data = datasets.load_from_disk(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_DATA_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTokenized data: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtokenized_data\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Define compute metrics\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(pred):\n",
      "        labels = pred.label_ids\n",
      "        preds = pred.predictions.argmax(-\u001b[34m1\u001b[39;49;00m)\n",
      "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\u001b[33m'\u001b[39;49;00m\u001b[33mmacro\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        acc = accuracy_score(labels, preds)\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: acc, \u001b[33m'\u001b[39;49;00m\u001b[33mf1_macro\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: f1, \u001b[33m'\u001b[39;49;00m\u001b[33mprecision\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: precision, \u001b[33m'\u001b[39;49;00m\u001b[33mrecall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: recall}\n",
      "    \n",
      "    \u001b[37m# Fine-tune \u001b[39;49;00m\n",
      "    training_args = TrainingArguments(output_dir=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                      overwrite_output_dir=\u001b[34mTrue\u001b[39;49;00m, \n",
      "                                      optim=\u001b[33m'\u001b[39;49;00m\u001b[33madamw_torch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                      per_device_train_batch_size=BATCH_SIZE, \n",
      "                                      per_device_eval_batch_size=BATCH_SIZE, \n",
      "                                      evaluation_strategy=\u001b[33m'\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                      num_train_epochs=TRAIN_EPOCHS,  \n",
      "                                      save_steps=SAVE_STEPS,\n",
      "                                      save_total_limit=SAVE_TOTAL_LIMIT)\n",
      "    \n",
      "    trainer = Trainer(model=model, \n",
      "                      args=training_args, \n",
      "                      train_dataset=tokenized_data[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                      eval_dataset=tokenized_data[\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                      tokenizer=tokenizer, \n",
      "                      compute_metrics=compute_metrics)\n",
      "    \n",
      "    \u001b[37m# Evaluate \u001b[39;49;00m\n",
      "    train_metrics = trainer.train()\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTrain metrics: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_metrics\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Evaluate validation set \u001b[39;49;00m\n",
      "    val_metrics = trainer.evaluate()\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mValidation metrics: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mval_metrics\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Evaluate test set (holdout)\u001b[39;49;00m\n",
      "    test_metrics = trainer.evaluate(eval_dataset=tokenized_data[\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mHoldout metrics: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtest_metrics\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_file_paths\u001b[39;49;00m(directory: \u001b[36mstr\u001b[39;49;00m) -> \u001b[36mlist\u001b[39;49;00m:\n",
      "        file_paths = [] \n",
      "        \u001b[34mfor\u001b[39;49;00m root, directories, files \u001b[35min\u001b[39;49;00m os.walk(directory):\n",
      "            \u001b[34mfor\u001b[39;49;00m file_name \u001b[35min\u001b[39;49;00m files:\n",
      "                file_path = os.path.join(root, file_name)\n",
      "                file_paths.append(file_path)  \n",
      "        \u001b[34mreturn\u001b[39;49;00m file_paths\n",
      "    \n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtar_artifacts\u001b[39;49;00m(local_artifacts_path: \u001b[36mstr\u001b[39;49;00m, tar_save_path: \u001b[36mstr\u001b[39;49;00m, tar_name: \u001b[36mstr\u001b[39;49;00m) -> \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(tar_save_path):\n",
      "            os.makedirs(tar_save_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        tar = tarfile.open(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtar_save_path\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtar_name\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mw:gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        file_paths = get_file_paths(local_artifacts_path)  \n",
      "        \u001b[34mfor\u001b[39;49;00m file_path \u001b[35min\u001b[39;49;00m file_paths:\n",
      "            file_ = file_path.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[-\u001b[34m1\u001b[39;49;00m]\n",
      "            \u001b[34mtry\u001b[39;49;00m:\n",
      "                tar.add(file_path, arcname=file_) \n",
      "            \u001b[34mexcept\u001b[39;49;00m \u001b[36mOSError\u001b[39;49;00m:\n",
      "                logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mIgnoring OSErrors during tar creation.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        tar.close()\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m current_host == master_host:\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(LOCAL_MODEL_DIR):\n",
      "            os.makedirs(LOCAL_MODEL_DIR, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(LOCAL_LABEL_DIR):\n",
      "            os.makedirs(LOCAL_LABEL_DIR, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "            \n",
      "        \u001b[37m# Download label mapping from s3 to local\u001b[39;49;00m\n",
      "        download(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/data/labels/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_LABEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, sm_session)\n",
      "    \n",
      "        \u001b[37m# Load label mapping for inference\u001b[39;49;00m\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_LABEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/label_map.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "            label2id = pickle.load(f)\n",
      "        \n",
      "        id2label = \u001b[36mdict\u001b[39;49;00m((\u001b[36mstr\u001b[39;49;00m(v), k) \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m label2id.items())\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mLabel mapping: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mid2label\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "        trainer.model.config.label2id = label2id\n",
      "        trainer.model.config.id2label = id2label\n",
      "        \n",
      "        \u001b[37m# Save finetuned model to local  \u001b[39;49;00m\n",
      "        trainer.save_model(LOCAL_MODEL_DIR)\n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m os.path.exists(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/pytorch_model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m \\\n",
      "            os.path.exists(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/training_args.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m \\\n",
      "            os.path.exists(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/config.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m \\\n",
      "            os.path.exists(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/special_tokens_map.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m \\\n",
      "            os.path.exists(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/tokenizer_config.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m \\\n",
      "            os.path.exists(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/tokenizer.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m \\\n",
      "            os.path.exists(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "            \n",
      "            \u001b[37m# Copy trained model from local directory of the training cluster to S3 \u001b[39;49;00m\n",
      "            logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCopying saved model from local to [s3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/model/finetuned-clf/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            upload(LOCAL_MODEL_DIR, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/model/finetuned-clf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, sm_session) \n",
      "        \n",
      "            \u001b[37m# Test model for inference\u001b[39;49;00m\n",
      "            classifier = pipeline(\u001b[33m'\u001b[39;49;00m\u001b[33msentiment-analysis\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, model=LOCAL_MODEL_DIR)\n",
      "            prediction = classifier(\u001b[33m'\u001b[39;49;00m\u001b[33mCovid pandemic is still raging in may parts of the world\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            logger.info(prediction)\n",
      "            \n",
      "            \u001b[37m# Save model as tar to local\u001b[39;49;00m\n",
      "            tar_artifacts(LOCAL_MODEL_DIR, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/model-tar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \n",
      "            \u001b[37m# Upload model tar to S3 from local\u001b[39;49;00m\n",
      "            logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCopying saved model tar from local to [s3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/model/finetuned-clf/model-tar/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            upload(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/model-tar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/model/finetuned-clf/model-tar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, sm_session) \n",
      "            \n",
      "            \u001b[37m# Save pipeline to local \u001b[39;49;00m\n",
      "            classifier.save_pretrained(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/pipeline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \n",
      "            \u001b[37m# Save pipeline as tar to local\u001b[39;49;00m\n",
      "            tar_artifacts(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/pipeline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/pipeline-tar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpipeline.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \n",
      "            \u001b[37m# Upload pipeline tar to S3 from local\u001b[39;49;00m\n",
      "            logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCopying saved pipeline tar from local to [s3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/model/finetuned-clf/pipeline-tar/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            upload(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mLOCAL_MODEL_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/pipeline-tar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/model/finetuned-clf/pipeline-tar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, sm_session)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./src/finetune_clf_oob.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the estimator \n",
    "\n",
    "* Documentation on SageMaker HuggingFace Estimator can be found [here](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {'train': f's3://{S3_BUCKET}/data'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512  # Context size for BERT tokenizer \n",
    "CHUNK_SIZE = 128  \n",
    "TRAIN_EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "REGION = 'us-east-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {'s3_bucket': S3_BUCKET, \n",
    "                   'max_len': MAX_LENGTH,\n",
    "                   'chunk_size': CHUNK_SIZE,\n",
    "                   'num_train_epochs': TRAIN_EPOCHS, \n",
    "                   'per_device_train_batch_size': BATCH_SIZE, \n",
    "                   'region': REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTRIBUTION_STRATEGY = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point=ENTRY_POINT, \n",
    "                                    source_dir=SOURCE_DIR, \n",
    "                                    role=ROLE, \n",
    "                                    instance_type=INSTANCE_TYPE, \n",
    "                                    instance_count=INSTANCE_COUNT,\n",
    "                                    volume_size=EBS_VOLUME_SIZE,\n",
    "                                    hyperparameters=HYPERPARAMETERS,\n",
    "                                    distribution=DISTRIBUTION_STRATEGY,\n",
    "                                    transformers_version=TRANSFORMERS_VERSION, \n",
    "                                    pytorch_version=PYTORCH_VERSION, \n",
    "                                    py_version=PYTHON_VERSION, \n",
    "                                    disable_profiler=True,\n",
    "                                    debugger_hook_config=False, \n",
    "                                    base_job_name=BASE_JOB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating training-job with name: hf-sm-clf-oob-2022-09-23-18-26-23-514\n",
      "train request: {\n",
      "    \"AlgorithmSpecification\": {\n",
      "        \"TrainingInputMode\": \"File\",\n",
      "        \"TrainingImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\",\n",
      "        \"EnableSageMakerMetricsTimeSeries\": true\n",
      "    },\n",
      "    \"OutputDataConfig\": {\n",
      "        \"S3OutputPath\": \"s3://sagemaker-us-east-1-119174016168/\"\n",
      "    },\n",
      "    \"TrainingJobName\": \"hf-sm-clf-oob-2022-09-23-18-26-23-514\",\n",
      "    \"StoppingCondition\": {\n",
      "        \"MaxRuntimeInSeconds\": 86400\n",
      "    },\n",
      "    \"ResourceConfig\": {\n",
      "        \"VolumeSizeInGB\": 1024,\n",
      "        \"InstanceCount\": 4,\n",
      "        \"InstanceType\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"RoleArn\": \"arn:aws:iam::119174016168:role/service-role/AmazonSageMaker-ExecutionRole-20211014T093628\",\n",
      "    \"InputDataConfig\": [\n",
      "        {\n",
      "            \"DataSource\": {\n",
      "                \"S3DataSource\": {\n",
      "                    \"S3DataType\": \"S3Prefix\",\n",
      "                    \"S3Uri\": \"s3://sagemaker-us-east-1-119174016168/data\",\n",
      "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
      "                }\n",
      "            },\n",
      "            \"ChannelName\": \"train\"\n",
      "        }\n",
      "    ],\n",
      "    \"HyperParameters\": {\n",
      "        \"s3_bucket\": \"\\\"sagemaker-us-east-1-119174016168\\\"\",\n",
      "        \"max_len\": \"512\",\n",
      "        \"chunk_size\": \"128\",\n",
      "        \"num_train_epochs\": \"2\",\n",
      "        \"per_device_train_batch_size\": \"8\",\n",
      "        \"region\": \"\\\"us-east-1\\\"\",\n",
      "        \"sagemaker_submit_directory\": \"\\\"s3://sagemaker-us-east-1-119174016168/hf-sm-clf-oob-2022-09-23-18-26-23-514/source/sourcedir.tar.gz\\\"\",\n",
      "        \"sagemaker_program\": \"\\\"finetune_clf_oob.py\\\"\",\n",
      "        \"sagemaker_container_log_level\": \"20\",\n",
      "        \"sagemaker_job_name\": \"\\\"hf-sm-clf-oob-2022-09-23-18-26-23-514\\\"\",\n",
      "        \"sagemaker_region\": \"\\\"us-east-1\\\"\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": \"true\",\n",
      "        \"sagemaker_instance_type\": \"\\\"ml.p3.16xlarge\\\"\",\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\\\"\\\"\"\n",
      "    },\n",
      "    \"Environment\": {\n",
      "        \"USE_SMDEBUG\": \"0\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(DATA, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
