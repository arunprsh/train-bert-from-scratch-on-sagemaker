{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune for Multi-class Classification using OOB BERT MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip install sagemaker==2.100.0\n",
    "!pip install jedi==0.17  # this is a requirement for pygmentize to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "import sagemaker\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log versions of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using SageMaker: 2.100.0]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using SageMaker: {sagemaker.__version__}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "ROLE = get_execution_role()\n",
    "S3_BUCKET = session.default_bucket()\n",
    "ENTRY_POINT = 'fine_tune.py'\n",
    "SOURCE_DIR = './src'\n",
    "INSTANCE_TYPE = 'ml.p3.16xlarge'\n",
    "INSTANCE_COUNT = 4\n",
    "EBS_VOLUME_SIZE = 1024\n",
    "TRANSFORMERS_VERSION = '4.17.0'\n",
    "PYTORCH_VERSION = '1.10.2'\n",
    "PYTHON_VERSION = 'py38'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S3 bucket = sagemaker-us-east-1-119174016168\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'S3 bucket = {S3_BUCKET}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertForSequenceClassification\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataCollatorWithPadding\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LabelEncoder\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainingArguments\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertTokenizerFast\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_dataset\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pipeline \n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Trainer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DatasetDict\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# Setup logging\u001b[39;49;00m\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logging.basicConfig(level=logging.getLevelName(\u001b[33m'\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \n",
      "                    handlers=[logging.StreamHandler(sys.stdout)], \n",
      "                    \u001b[36mformat\u001b[39;49;00m=\u001b[33m'\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# Log versions of dependencies\u001b[39;49;00m\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Transformers: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtransformers.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using SageMaker: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msagemaker.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Datasets: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdatasets.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Torch: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtorch.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \u001b[37m# Hyperparameters sent by the client (Studio notebook with the driver code to launch training)\u001b[39;49;00m\n",
      "    \u001b[37m# are passed as command-line arguments to the training script\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mHandling command line arguments\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    args, _ = parser.parse_known_args()\n",
      "    \n",
      "    CURRENT_HOST = args.current_host\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m * \u001b[34m20\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.listdir(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m * \u001b[34m20\u001b[39;49;00m)\n",
      "    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mCURRENT_HOST\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mInput directory = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mModel directory = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.model_dir\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTrain directory = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.train_dir\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Download saved custom vocabulary file from S3 to local input path of the training cluster\u001b[39;49;00m\n",
      "    s3 = boto3.resource(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    bucket = s3.Bucket(\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-us-east-1-119174016168\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    path = os.path.join(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mvocab\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(path):\n",
      "        os.makedirs(path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpath\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m data:\n",
      "        bucket.download_fileobj(\u001b[33m'\u001b[39;49;00m\u001b[33mvocab/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, data)\n",
      "    \n",
      "    \u001b[37m# Re-create BERT tokenizer\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mRe-creating BERT tokenizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    tokenizer = BertTokenizerFast.from_pretrained(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    tokenizer.model_max_length = \u001b[34m512\u001b[39;49;00m\n",
      "    tokenizer.init_kwargs[\u001b[33m'\u001b[39;49;00m\u001b[33mmodel_max_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[34m512\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# Load and split dataset \u001b[39;49;00m\n",
      "    data = datasets.load_dataset(\u001b[33m'\u001b[39;49;00m\u001b[33mcsv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                 data_files=\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.train_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/covid_articles_clf_data.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                 column_names=[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                                 delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                 split=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                 cache_dir=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    train_dev_test = data.train_test_split(shuffle=\u001b[34mTrue\u001b[39;49;00m, seed=\u001b[34m123\u001b[39;49;00m, test_size=\u001b[34m0.1\u001b[39;49;00m)\n",
      "    dev_test = train_dev_test[\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].train_test_split(shuffle=\u001b[34mTrue\u001b[39;49;00m, seed=\u001b[34m123\u001b[39;49;00m, test_size=\u001b[34m0.5\u001b[39;49;00m)\n",
      "    data_splits = DatasetDict({\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: train_dev_test[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                               \u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: dev_test[\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                               \u001b[33m'\u001b[39;49;00m\u001b[33mdev\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: dev_test[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]})\n",
      "    \n",
      "    \u001b[37m# Tokenize data splits\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mpreprocess_function\u001b[39;49;00m(examples):\n",
      "        \u001b[34mreturn\u001b[39;49;00m tokenizer(examples[\u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], truncation=\u001b[34mTrue\u001b[39;49;00m, padding=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    tokenized_data = data_splits.map(preprocess_function, batched=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Create data collator\u001b[39;49;00m\n",
      "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
      "    \n",
      "    \n",
      "    path = os.path.join(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbert\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(path):\n",
      "        os.makedirs(path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpath\u001b[33m}\u001b[39;49;00m\u001b[33m/pytorch_model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m data:\n",
      "        bucket.download_fileobj(\u001b[33m'\u001b[39;49;00m\u001b[33mmodel/pytorch_model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, data)\n",
      "    \n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    with open(f'{path}/training_args.bin', 'wb') as data:\u001b[39;49;00m\n",
      "\u001b[33m        bucket.download_fileobj('model/training_args.bin', data)\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpath\u001b[33m}\u001b[39;49;00m\u001b[33m/config.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m data:\n",
      "        bucket.download_fileobj(\u001b[33m'\u001b[39;49;00m\u001b[33mmodel/config.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, data)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \u001b[37m# [IMPORTANT] Copy vocab.txt to local model directory - this is needed to re-create the trained model\u001b[39;49;00m\n",
      "    shutil.copyfile(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/bert/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \u001b[37m# Load BERT model\u001b[39;49;00m\n",
      "    model = BertForSequenceClassification.from_pretrained(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/bert/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, num_labels=\u001b[34m5\u001b[39;49;00m, force_download=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Set training args\u001b[39;49;00m\n",
      "    training_args = TrainingArguments(output_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mtmp/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                      learning_rate=\u001b[34m2e-5\u001b[39;49;00m, \n",
      "                                      per_device_train_batch_size=\u001b[34m16\u001b[39;49;00m, \n",
      "                                      per_device_eval_batch_size=\u001b[34m16\u001b[39;49;00m, \n",
      "                                      num_train_epochs=\u001b[34m5\u001b[39;49;00m,  \n",
      "                                      weight_decay=\u001b[34m0.01\u001b[39;49;00m, \n",
      "                                      save_total_limit=\u001b[34m2\u001b[39;49;00m, \n",
      "                                      save_strategy=\u001b[33m'\u001b[39;49;00m\u001b[33mno\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                      load_best_model_at_end=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \u001b[37m# Fine-tune the model\u001b[39;49;00m\n",
      "    trainer = Trainer(model=model, \n",
      "                      args=training_args, \n",
      "                      train_dataset=tokenized_data[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                      eval_dataset=tokenized_data[\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                      tokenizer=tokenizer, \n",
      "                      data_collator=data_collator)\n",
      "    trainer.train()\n",
      "    \n",
      "    \u001b[37m# Save fine-tuned model to local model directory\u001b[39;49;00m\n",
      "    trainer.save_model(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.model_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/fine-tuned/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    time.sleep(\u001b[34m120\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.listdir(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.model_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/fine-tuned/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \n",
      "    \u001b[37m# Copy trained model from local directory of the training cluster to S3 \u001b[39;49;00m\n",
      "    s3 = boto3.resource(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    s3.meta.client.upload_file(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.model_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/fine-tuned/pytorch_model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                               \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-us-east-1-119174016168\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                               \u001b[33m'\u001b[39;49;00m\u001b[33mmodel/fine-tuned-clf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + \u001b[33m'\u001b[39;49;00m\u001b[33mpytorch_model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    s3.meta.client.upload_file(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.model_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/fine-tuned/config.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                               \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-us-east-1-119174016168\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                               \u001b[33m'\u001b[39;49;00m\u001b[33mmodel/fine-tuned-clf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + \u001b[33m'\u001b[39;49;00m\u001b[33mconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Load the model and create a classification pipeline \u001b[39;49;00m\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mEvaluate fine-tuned model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    classifier = pipeline(\u001b[33m'\u001b[39;49;00m\u001b[33msentiment-analysis\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, model=\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.model_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/fine-tuned/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    result = classifier(\u001b[33m'\u001b[39;49;00m\u001b[33mI hate you\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./src/fine_tune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the estimator \n",
    "\n",
    "* Documentation on SageMaker HuggingFace Estimator can be found [here](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {'train': f's3://{S3_BUCKET}/data'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512  # Context size for BERT tokenizer \n",
    "CHUNK_SIZE = 128  \n",
    "TRAIN_EPOCHS = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {'s3_bucket': S3_BUCKET, \n",
    "                   'max_len': MAX_LENGTH,\n",
    "                   'chunk_size': CHUNK_SIZE,\n",
    "                   'num_train_epochs': TRAIN_EPOCHS, \n",
    "                   'per_device_train_batch_size': BATCH_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTRIBUTION_STRATEGY = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point=ENTRY_POINT, \n",
    "                                    source_dir=SOURCE_DIR, \n",
    "                                    role=ROLE, \n",
    "                                    instance_type=INSTANCE_TYPE, \n",
    "                                    instance_count=INSTANCE_COUNT,\n",
    "                                    volume_size=EBS_VOLUME_SIZE,\n",
    "                                    hyperparameters=HYPERPARAMETERS,\n",
    "                                    distribution=DISTRIBUTION_STRATEGY,\n",
    "                                    transformers_version=TRANSFORMERS_VERSION, \n",
    "                                    pytorch_version=PYTORCH_VERSION, \n",
    "                                    py_version=PYTHON_VERSION, \n",
    "                                    disable_profiler=True,\n",
    "                                    debugger_hook_config=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.fit(DATA, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
