{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe513c3-b3e7-4bcb-997a-4e2f4af1000f",
   "metadata": {},
   "source": [
    "#### Prerequisites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930c1331-a6f3-4dc4-9d83-2c73e58210b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip install sagemaker==2.100.0\n",
    "!pip install jedi==0.17  # this is a requirement for pygmentize to work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc60fb-7421-4514-9e5d-c4a6462dda46",
   "metadata": {},
   "source": [
    "#### Imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd341ee-6cac-4b64-9937-f9df11b245f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "import sagemaker\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16910da-df2f-44c6-bb15-c8b1af14f8c6",
   "metadata": {},
   "source": [
    "##### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d78c31-cfc9-4215-a0fc-95151638becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a41e83-02be-42fd-afc6-b85e3a6ded9a",
   "metadata": {},
   "source": [
    "##### Log versions of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd797da-165e-4ac9-8c6f-62ee2e15d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using SageMaker: 2.100.0]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using SageMaker: {sagemaker.__version__}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c086b-22ef-494c-9620-c02b11f1b060",
   "metadata": {},
   "source": [
    "#### Essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52847b7-2144-4bb6-9b6d-da76bec55993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S3 bucket = sagemaker-us-east-1-119174016168\n"
     ]
    }
   ],
   "source": [
    "session = Session()\n",
    "ROLE = get_execution_role()\n",
    "S3_BUCKET = session.default_bucket()\n",
    "logger.info(f'S3 bucket = {S3_BUCKET}')\n",
    "\n",
    "ENTRY_POINT = 'train.py'\n",
    "SOURCE_DIR = './src'\n",
    "INSTANCE_TYPE = 'ml.p4d.24xlarge'\n",
    "INSTANCE_COUNT = 4\n",
    "EBS_VOLUME_SIZE = 1024\n",
    "TRANSFORMERS_VERSION = '4.17.0'\n",
    "PYTORCH_VERSION = '1.10.2'\n",
    "PYTHON_VERSION = 'py38'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7a5a8-3ca4-4150-a532-723d11f50b0e",
   "metadata": {},
   "source": [
    "#### View training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f225b6-ad53-4603-95d6-0b974274bc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataCollatorForLanguageModeling\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainingArguments\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertTokenizerFast\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertForMaskedLM\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pipeline \n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_dataset\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Trainer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DatasetDict\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Setup logging\u001b[39;49;00m\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logging.basicConfig(level=logging.getLevelName(\u001b[33m'\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \n",
      "                    handlers=[logging.StreamHandler(sys.stdout)], \n",
      "                    \u001b[36mformat\u001b[39;49;00m=\u001b[33m'\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# Log versions of dependencies\u001b[39;49;00m\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Transformers: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtransformers.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using SageMaker: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msagemaker.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Datasets: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdatasets.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Torch: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtorch.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Pandas: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpd.__version__\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# Essentials \u001b[39;49;00m\n",
      "config = BertConfig()\n",
      "s3 = boto3.resource(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "s3_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mParsing command line arguments\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--master_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSMDATAPARALLEL_SERVER_ADDR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[37m# [IMPORTANT] Hyperparameters sent by the client (Studio notebook with the driver code to launch training) \u001b[39;49;00m\n",
      "    \u001b[37m# are passed as command-line arguments to the training script\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--s3_bucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--chunk_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_train_epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--per_device_train_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    \n",
      "    args, _ = parser.parse_known_args()\n",
      "    current_host = args.current_host\n",
      "    master_host = args.master_host\n",
      "    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcurrent_host\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mMaster host = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmaster_host\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    S3_BUCKET = args.s3_bucket\n",
      "    MAX_LENGTH = args.max_len\n",
      "    CHUNK_SIZE = args.chunk_size\n",
      "    TRAIN_EPOCHS = args.num_train_epochs\n",
      "    BATCH_SIZE = args.per_device_train_batch_size\n",
      "    SAVE_STEPS = \u001b[34m10000\u001b[39;49;00m\n",
      "    SAVE_TOTAL_LIMIT = \u001b[34m2\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# Download saved custom vocabulary file from S3 to local input path of the training cluster\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mDownloading custom vocabulary from [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab/] to [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    bucket = s3.Bucket(S3_BUCKET)\n",
      "    path = os.path.join(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mvocab\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(path):\n",
      "        os.makedirs(path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpath\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m data:\n",
      "        bucket.download_fileobj(\u001b[33m'\u001b[39;49;00m\u001b[33mvocab/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, data)\n",
      "        \n",
      "    \u001b[37m# Copy preprocessed datasets from S3 to local EBS volume (cache dir)\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mDownloading preprocessed datasets from [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/data/processed/] to [/tmp/cache/data/processed/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_bucket_content\u001b[39;49;00m(bucket, prefix=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "        files = []\n",
      "        folders = []\n",
      "        default_kwargs = {\u001b[33m'\u001b[39;49;00m\u001b[33mBucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: bucket, \u001b[33m'\u001b[39;49;00m\u001b[33mPrefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: prefix}\n",
      "        next_token = \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "        \u001b[34mwhile\u001b[39;49;00m next_token \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "            updated_kwargs = default_kwargs.copy()\n",
      "            \u001b[34mif\u001b[39;49;00m next_token != \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "                updated_kwargs[\u001b[33m'\u001b[39;49;00m\u001b[33mContinuationToken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = next_token\n",
      "            response = s3_client.list_objects_v2(**default_kwargs)\n",
      "            contents = response.get(\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \u001b[34mfor\u001b[39;49;00m result \u001b[35min\u001b[39;49;00m contents:\n",
      "                key = result.get(\u001b[33m'\u001b[39;49;00m\u001b[33mKey\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                \u001b[34mif\u001b[39;49;00m key[-\u001b[34m1\u001b[39;49;00m] == \u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "                    folders.append(key)\n",
      "                \u001b[34melse\u001b[39;49;00m:\n",
      "                    files.append(key)\n",
      "            next_token = response.get(\u001b[33m'\u001b[39;49;00m\u001b[33mNextContinuationToken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m files, folders\n",
      "    \n",
      "    files, folders = get_bucket_content(S3_BUCKET, \u001b[33m'\u001b[39;49;00m\u001b[33mdata/processed/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcopy_to_local_from_s3\u001b[39;49;00m(bucket: \u001b[36mstr\u001b[39;49;00m, local_path: \u001b[36mstr\u001b[39;49;00m, files: \u001b[36mlist\u001b[39;49;00m, folders: \u001b[36mlist\u001b[39;49;00m) -> \u001b[34mNone\u001b[39;49;00m:\n",
      "        local_path = Path(local_path)\n",
      "        \u001b[34mfor\u001b[39;49;00m folder \u001b[35min\u001b[39;49;00m folders:\n",
      "            folder_path = Path.joinpath(local_path, folder)\n",
      "            folder_path.mkdir(parents=\u001b[34mTrue\u001b[39;49;00m, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[34mfor\u001b[39;49;00m file_name \u001b[35min\u001b[39;49;00m files:\n",
      "            file_path = Path.joinpath(local_path, file_name)\n",
      "            file_path.parent.mkdir(parents=\u001b[34mTrue\u001b[39;49;00m, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "            s3_client.download_file(bucket, file_name, \u001b[36mstr\u001b[39;49;00m(file_path))\n",
      "\n",
      "\n",
      "    copy_to_local_from_s3(S3_BUCKET, \u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, files, folders)\n",
      "    \n",
      "    \n",
      "    \u001b[37m# Re-create BERT WordPiece tokenizer \u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mRe-creating BERT tokenizer using custom vocabulary from [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    tokenizer = BertTokenizerFast.from_pretrained(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, config=config)\n",
      "    tokenizer.model_max_length = MAX_LENGTH\n",
      "    tokenizer.init_kwargs[\u001b[33m'\u001b[39;49;00m\u001b[33mmodel_max_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = MAX_LENGTH\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTokenizer: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtokenizer\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Read dataset \u001b[39;49;00m\n",
      "    chunked_datasets = datasets.load_from_disk(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/data/processed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mChunked datasets: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mchunked_datasets\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "   \n",
      "    \u001b[37m# Create data collator\u001b[39;49;00m\n",
      "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
      "                                                    mlm=\u001b[34mTrue\u001b[39;49;00m, \n",
      "                                                    mlm_probability=\u001b[34m0.15\u001b[39;49;00m)\n",
      "        \n",
      "    \u001b[37m# Load MLM\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mLoading BertForMaskedLM model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    mlm = BertForMaskedLM(config=config)\n",
      "    \n",
      "    \u001b[37m# Train MLM\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTraining MLM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    training_args = TrainingArguments(output_dir=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                      overwrite_output_dir=\u001b[34mTrue\u001b[39;49;00m, \n",
      "                                      optim=\u001b[33m'\u001b[39;49;00m\u001b[33madamw_torch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                      num_train_epochs=TRAIN_EPOCHS,\n",
      "                                      per_device_train_batch_size=BATCH_SIZE,\n",
      "                                      evaluation_strategy=\u001b[33m'\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                      save_steps=SAVE_STEPS, \n",
      "                                      save_total_limit=SAVE_TOTAL_LIMIT)\n",
      "    trainer = Trainer(model=mlm, \n",
      "                      args=training_args, \n",
      "                      data_collator=data_collator,\n",
      "                      train_dataset=chunked_datasets[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "                      eval_dataset=chunked_datasets[\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    trainer.train()\n",
      "    \n",
      "    eval_results = trainer.evaluate()\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mPerplexity: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmath.exp(eval_results[\u001b[33m'\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[33m:\u001b[39;49;00m\u001b[33m.2f\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m current_host == master_host:\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/custom\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "            os.makedirs(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/custom\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[37m# Save trained model to local model directory\u001b[39;49;00m\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mSaving trained MLM to [/tmp/cache/model/custom/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        trainer.save_model(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/custom\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        logger.info(os.listdir(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/custom\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "        \u001b[34mif\u001b[39;49;00m os.path.exists(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/custom/pytorch_model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m os.path.exists(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/custom/config.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "            \u001b[37m# Copy trained model from local directory of the training cluster to S3 \u001b[39;49;00m\n",
      "            logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCopying saved model from local to [s3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/model/custom/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            s3.meta.client.upload_file(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/custom/pytorch_model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, S3_BUCKET, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel/custom/pytorch_model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            s3.meta.client.upload_file(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/custom/config.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, S3_BUCKET, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel/custom/config.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "            \u001b[37m# Copy vocab.txt to local model directory - this is needed to re-create the trained MLM\u001b[39;49;00m\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mCopying custom vocabulary to local model artifacts location to faciliate model evaluation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            shutil.copyfile(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/custom/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "            \u001b[37m# Copy vocab.txt to saved model artifacts location in S3\u001b[39;49;00m\n",
      "            logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCopying custom vocabulary from [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpath\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab.txt] to [s3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/model/custom/] for future stages of ML pipeline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            s3.meta.client.upload_file(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpath\u001b[33m}\u001b[39;49;00m\u001b[33m/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, S3_BUCKET, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel/custom/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "            \u001b[37m# Evaluate the trained model \u001b[39;49;00m\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mCreate fill-mask task pipeline to evaluate trained MLM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            fill_mask = pipeline(\u001b[33m'\u001b[39;49;00m\u001b[33mfill-mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, model=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/custom\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            df = pd.read_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mS3_BUCKET\u001b[33m}\u001b[39;49;00m\u001b[33m/data/eval/eval_mlm.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "            \u001b[34mfor\u001b[39;49;00m gt, masked_sentence \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(df.ground_truth.tolist(), df.masked.tolist()):\n",
      "                logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mGround Truth    : \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mgt\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mMasked sentence : \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmasked_sentence\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                predictions = fill_mask(masked_sentence, top_k=\u001b[34m3\u001b[39;49;00m)\n",
      "                \u001b[34mfor\u001b[39;49;00m i, prediction \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(predictions):\n",
      "                    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mRank: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mi+\u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m | \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m(prediction[\u001b[33m\"\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] * \u001b[34m100\u001b[39;49;00m)\u001b[33m:\u001b[39;49;00m\u001b[33m.2f\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m % | \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m[prediction[\u001b[33m\"\u001b[39;49;00m\u001b[33mtoken_str\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m * \u001b[34m10\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./src/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff0505-2701-4537-a7ac-403d8f572b81",
   "metadata": {},
   "source": [
    "#### Create the estimator \n",
    "\n",
    "* Documentation on SageMaker HuggingFace Estimator can be found [here](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e91f3385-b807-4a04-9237-44486fec8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {'train': f's3://{S3_BUCKET}/data'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18fab4cb-415a-4f50-afa5-7544119b8629",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512  # Context size for BERT tokenizer \n",
    "CHUNK_SIZE = 128  \n",
    "TRAIN_EPOCHS = 40\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82b0630f-5545-4874-a83c-4d592b4bf5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {'s3_bucket': S3_BUCKET, \n",
    "                   'max_len': MAX_LENGTH,\n",
    "                   'chunk_size': CHUNK_SIZE,\n",
    "                   'num_train_epochs': TRAIN_EPOCHS, \n",
    "                   'per_device_train_batch_size': BATCH_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90f209dd-c155-4e65-930f-25323eba0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTRIBUTION_STRATEGY = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96341f8c-cd18-4a29-b740-d1543916aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point=ENTRY_POINT, \n",
    "                                    source_dir=SOURCE_DIR, \n",
    "                                    role=ROLE, \n",
    "                                    instance_type=INSTANCE_TYPE, \n",
    "                                    instance_count=INSTANCE_COUNT,\n",
    "                                    volume_size=EBS_VOLUME_SIZE,\n",
    "                                    hyperparameters=HYPERPARAMETERS,\n",
    "                                    distribution=DISTRIBUTION_STRATEGY,\n",
    "                                    transformers_version=TRANSFORMERS_VERSION, \n",
    "                                    pytorch_version=PYTORCH_VERSION, \n",
    "                                    py_version=PYTHON_VERSION, \n",
    "                                    disable_profiler=True,\n",
    "                                    debugger_hook_config=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b98aac31-3ddb-4142-ba42-811947320a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating training-job with name: huggingface-pytorch-training-2022-08-25-21-38-33-886\n",
      "train request: {\n",
      "    \"AlgorithmSpecification\": {\n",
      "        \"TrainingInputMode\": \"File\",\n",
      "        \"TrainingImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\",\n",
      "        \"EnableSageMakerMetricsTimeSeries\": true\n",
      "    },\n",
      "    \"OutputDataConfig\": {\n",
      "        \"S3OutputPath\": \"s3://sagemaker-us-east-1-119174016168/\"\n",
      "    },\n",
      "    \"TrainingJobName\": \"huggingface-pytorch-training-2022-08-25-21-38-33-886\",\n",
      "    \"StoppingCondition\": {\n",
      "        \"MaxRuntimeInSeconds\": 86400\n",
      "    },\n",
      "    \"ResourceConfig\": {\n",
      "        \"VolumeSizeInGB\": 1024,\n",
      "        \"InstanceCount\": 4,\n",
      "        \"InstanceType\": \"ml.p4d.24xlarge\"\n",
      "    },\n",
      "    \"RoleArn\": \"arn:aws:iam::119174016168:role/service-role/AmazonSageMaker-ExecutionRole-20211014T093628\",\n",
      "    \"InputDataConfig\": [\n",
      "        {\n",
      "            \"DataSource\": {\n",
      "                \"S3DataSource\": {\n",
      "                    \"S3DataType\": \"S3Prefix\",\n",
      "                    \"S3Uri\": \"s3://sagemaker-us-east-1-119174016168/data\",\n",
      "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
      "                }\n",
      "            },\n",
      "            \"ChannelName\": \"train\"\n",
      "        }\n",
      "    ],\n",
      "    \"HyperParameters\": {\n",
      "        \"s3_bucket\": \"\\\"sagemaker-us-east-1-119174016168\\\"\",\n",
      "        \"max_len\": \"512\",\n",
      "        \"chunk_size\": \"128\",\n",
      "        \"num_train_epochs\": \"40\",\n",
      "        \"per_device_train_batch_size\": \"32\",\n",
      "        \"sagemaker_submit_directory\": \"\\\"s3://sagemaker-us-east-1-119174016168/huggingface-pytorch-training-2022-08-25-21-38-33-886/source/sourcedir.tar.gz\\\"\",\n",
      "        \"sagemaker_program\": \"\\\"train.py\\\"\",\n",
      "        \"sagemaker_container_log_level\": \"20\",\n",
      "        \"sagemaker_job_name\": \"\\\"huggingface-pytorch-training-2022-08-25-21-38-33-886\\\"\",\n",
      "        \"sagemaker_region\": \"\\\"us-east-1\\\"\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": \"true\",\n",
      "        \"sagemaker_instance_type\": \"\\\"ml.p4d.24xlarge\\\"\",\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\\\"\\\"\"\n",
      "    },\n",
      "    \"Environment\": {\n",
      "        \"USE_SMDEBUG\": \"0\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(DATA, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad713a0b-8532-4ff2-ac58-3a3ca96763dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
