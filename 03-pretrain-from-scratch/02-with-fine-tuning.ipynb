{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6bce0b3-3f4a-472a-8a8b-735f0e6c00dd",
   "metadata": {},
   "source": [
    "## `Fine-tune` original BERT with our COVID articles and take the `fill mask` test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318287d2-f640-4029-a8bf-0a31c6708eb2",
   "metadata": {},
   "source": [
    "#### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572e5422-9f87-416f-9bc6-1c4df02998f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip install sagemaker==2.100.0\n",
    "!pip install jedi==0.17  # this is a requirement for pygmentize to work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e589a051-d469-4b92-95a0-d50dea3d8cf0",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6664bef4-a104-46f6-beaa-16ded94f622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "import sagemaker\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c2d009-397a-4143-a508-72024c39fc7f",
   "metadata": {},
   "source": [
    "##### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024eaa36-913a-4fec-add3-e355d950829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cddfee-293e-46f2-8789-1593c9ca66fa",
   "metadata": {},
   "source": [
    "##### Log versions of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b194d0-84c2-4a30-a7dc-62100cb723f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using SageMaker: 2.100.0]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using SageMaker: {sagemaker.__version__}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5ac5a-7d77-40ec-86fb-51d3d5d3585f",
   "metadata": {},
   "source": [
    "#### Essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9dff828-cc10-4fd5-8f9d-83aa14a595c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S3 bucket = sagemaker-us-east-1-119174016168\n"
     ]
    }
   ],
   "source": [
    "session = Session()\n",
    "ROLE = get_execution_role()\n",
    "S3_BUCKET = session.default_bucket()\n",
    "logger.info(f'S3 bucket = {S3_BUCKET}')\n",
    "\n",
    "ENTRY_POINT = 'fine_tune.py'\n",
    "SOURCE_DIR = './src'\n",
    "INSTANCE_TYPE = 'ml.p4d.24xlarge'\n",
    "INSTANCE_COUNT = 4\n",
    "EBS_VOLUME_SIZE = 1024\n",
    "TRANSFORMERS_VERSION = '4.17.0'\n",
    "PYTORCH_VERSION = '1.10.2'\n",
    "PYTHON_VERSION = 'py38'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14acc3d2-96d9-497b-aaec-19a932b9aa08",
   "metadata": {},
   "source": [
    "#### Download vocabulary for original BERT base uncased to local `vocab` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f97bd1fa-7554-4f3e-8ff7-da77d51498d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!wget -q https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt -O ./vocab/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05f9a42-eec6-4115-8318-535f58b01c01",
   "metadata": {},
   "source": [
    "##### Copy vocabulary file from local `vocab` directory to S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e50686c1-f9df-462c-accd-715854ed516d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: vocab/vocab.txt to s3://sagemaker-us-east-1-119174016168/data/bert/vocab/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./vocab/vocab.txt s3://{S3_BUCKET}/data/bert/vocab/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5011f56c-9d6b-4cd5-b206-bfb2726cee40",
   "metadata": {},
   "source": [
    "#### View training script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ada6b5b-cac6-4d75-8fe4-b7858f5c9639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataCollatorForLanguageModeling\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainingArguments\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertTokenizerFast\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertForMaskedLM\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pipeline \n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_dataset\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Trainer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DatasetDict\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Setup logging\u001b[39;49;00m\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logging.basicConfig(level=logging.getLevelName(\u001b[33m'\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \n",
      "                    handlers=[logging.StreamHandler(sys.stdout)], \n",
      "                    \u001b[36mformat\u001b[39;49;00m=\u001b[33m'\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# Log versions of dependencies\u001b[39;49;00m\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Transformers: \u001b[39;49;00m\u001b[33m{transformers.__version__}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using SageMaker: \u001b[39;49;00m\u001b[33m{sagemaker.__version__}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Datasets: \u001b[39;49;00m\u001b[33m{datasets.__version__}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Torch: \u001b[39;49;00m\u001b[33m{torch.__version__}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[Using Pandas: \u001b[39;49;00m\u001b[33m{pd.__version__}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# Essentials \u001b[39;49;00m\n",
      "config = BertConfig()\n",
      "s3 = boto3.resource(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "s3_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mParsing command line arguments\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--master_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSMDATAPARALLEL_SERVER_ADDR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[37m# [IMPORTANT] Hyperparameters sent by the client (Studio notebook with the driver code to launch training) \u001b[39;49;00m\n",
      "    \u001b[37m# are passed as command-line arguments to the training script\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--s3_bucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--chunk_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_train_epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--per_device_train_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    \n",
      "    args, _ = parser.parse_known_args()\n",
      "    current_host = args.current_host\n",
      "    master_host = args.master_host\n",
      "    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host = \u001b[39;49;00m\u001b[33m{current_host}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mMaster host = \u001b[39;49;00m\u001b[33m{master_host}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    S3_BUCKET = args.s3_bucket\n",
      "    MAX_LENGTH = args.max_len\n",
      "    CHUNK_SIZE = args.chunk_size\n",
      "    TRAIN_EPOCHS = args.num_train_epochs\n",
      "    BATCH_SIZE = args.per_device_train_batch_size\n",
      "    SAVE_STEPS = \u001b[34m10000\u001b[39;49;00m\n",
      "    SAVE_TOTAL_LIMIT = \u001b[34m2\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# Download saved original BERT vocabulary file from S3 to local input path of the training cluster\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mDownloading custom vocabulary from [\u001b[39;49;00m\u001b[33m{S3_BUCKET}\u001b[39;49;00m\u001b[33m/data/bert/vocab/] to [\u001b[39;49;00m\u001b[33m{args.input_dir}\u001b[39;49;00m\u001b[33m/vocab/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    bucket = s3.Bucket(S3_BUCKET)\n",
      "    path = os.path.join(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{args.input_dir}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mvocab\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(path):\n",
      "        os.makedirs(path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{path}\u001b[39;49;00m\u001b[33m/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m file_:\n",
      "        bucket.download_fileobj(\u001b[33m'\u001b[39;49;00m\u001b[33mdata/bert/vocab/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, file_)\n",
      "        \n",
      "    \u001b[37m# Copy preprocessed datasets from S3 to local EBS volume (cache dir)\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mDownloading preprocessed datasets from [\u001b[39;49;00m\u001b[33m{S3_BUCKET}\u001b[39;49;00m\u001b[33m/data/processed/] to [/tmp/cache/data/processed/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_bucket_content\u001b[39;49;00m(bucket, prefix=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "        files = []\n",
      "        folders = []\n",
      "        default_kwargs = {\u001b[33m'\u001b[39;49;00m\u001b[33mBucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: bucket, \u001b[33m'\u001b[39;49;00m\u001b[33mPrefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: prefix}\n",
      "        next_token = \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "        \u001b[34mwhile\u001b[39;49;00m next_token \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "            updated_kwargs = default_kwargs.copy()\n",
      "            \u001b[34mif\u001b[39;49;00m next_token != \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "                updated_kwargs[\u001b[33m'\u001b[39;49;00m\u001b[33mContinuationToken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = next_token\n",
      "            response = s3_client.list_objects_v2(**default_kwargs)\n",
      "            contents = response.get(\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \u001b[34mfor\u001b[39;49;00m result \u001b[35min\u001b[39;49;00m contents:\n",
      "                key = result.get(\u001b[33m'\u001b[39;49;00m\u001b[33mKey\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                \u001b[34mif\u001b[39;49;00m key[-\u001b[34m1\u001b[39;49;00m] == \u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "                    folders.append(key)\n",
      "                \u001b[34melse\u001b[39;49;00m:\n",
      "                    files.append(key)\n",
      "            next_token = response.get(\u001b[33m'\u001b[39;49;00m\u001b[33mNextContinuationToken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m files, folders\n",
      "    \n",
      "    files, folders = get_bucket_content(S3_BUCKET, \u001b[33m'\u001b[39;49;00m\u001b[33mdata/processed/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcopy_to_local_from_s3\u001b[39;49;00m(bucket: \u001b[36mstr\u001b[39;49;00m, local_path: \u001b[36mstr\u001b[39;49;00m, files: \u001b[36mlist\u001b[39;49;00m, folders: \u001b[36mlist\u001b[39;49;00m) -> \u001b[34mNone\u001b[39;49;00m:\n",
      "        local_path = Path(local_path)\n",
      "        \u001b[34mfor\u001b[39;49;00m folder \u001b[35min\u001b[39;49;00m folders:\n",
      "            folder_path = Path.joinpath(local_path, folder)\n",
      "            folder_path.mkdir(parents=\u001b[34mTrue\u001b[39;49;00m, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[34mfor\u001b[39;49;00m file_name \u001b[35min\u001b[39;49;00m files:\n",
      "            file_path = Path.joinpath(local_path, file_name)\n",
      "            file_path.parent.mkdir(parents=\u001b[34mTrue\u001b[39;49;00m, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "            s3_client.download_file(bucket, file_name, \u001b[36mstr\u001b[39;49;00m(file_path))\n",
      "\n",
      "\n",
      "    copy_to_local_from_s3(S3_BUCKET, \u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, files, folders)\n",
      "    \n",
      "    \n",
      "    \u001b[37m# Re-create BERT WordPiece tokenizer \u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mRe-creating BERT tokenizer using custom vocabulary from [\u001b[39;49;00m\u001b[33m{args.input_dir}\u001b[39;49;00m\u001b[33m/vocab/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    tokenizer = BertTokenizerFast.from_pretrained(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{args.input_dir}\u001b[39;49;00m\u001b[33m/vocab/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, config=config)\n",
      "    tokenizer.model_max_length = MAX_LENGTH\n",
      "    tokenizer.init_kwargs[\u001b[33m'\u001b[39;49;00m\u001b[33mmodel_max_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = MAX_LENGTH\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTokenizer: \u001b[39;49;00m\u001b[33m{tokenizer}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Read dataset \u001b[39;49;00m\n",
      "    chunked_datasets = datasets.load_from_disk(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/data/processed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mChunked datasets: \u001b[39;49;00m\u001b[33m{chunked_datasets}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "   \n",
      "    \u001b[37m# Create data collator\u001b[39;49;00m\n",
      "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
      "                                                    mlm=\u001b[34mTrue\u001b[39;49;00m, \n",
      "                                                    mlm_probability=\u001b[34m0.15\u001b[39;49;00m)\n",
      "        \n",
      "    \u001b[37m# Load MLM\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mLoading BertForMaskedLM model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    mlm = BertForMaskedLM(config=config)\n",
      "    \n",
      "    \u001b[37m# Train MLM\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTraining MLM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    training_args = TrainingArguments(output_dir=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                      overwrite_output_dir=\u001b[34mTrue\u001b[39;49;00m, \n",
      "                                      optim=\u001b[33m'\u001b[39;49;00m\u001b[33madamw_torch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                      num_train_epochs=TRAIN_EPOCHS,\n",
      "                                      per_device_train_batch_size=BATCH_SIZE,\n",
      "                                      evaluation_strategy=\u001b[33m'\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                      save_steps=SAVE_STEPS, \n",
      "                                      save_total_limit=SAVE_TOTAL_LIMIT)\n",
      "    trainer = Trainer(model=mlm, \n",
      "                      args=training_args, \n",
      "                      data_collator=data_collator,\n",
      "                      train_dataset=chunked_datasets[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "                      eval_dataset=chunked_datasets[\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    trainer.train()\n",
      "    \n",
      "    eval_results = trainer.evaluate()\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mPerplexity: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mmath.exp(eval_results[\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m]):.2f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m current_host == master_host:\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/finetuned\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "            os.makedirs(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/finetuned\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[37m# Save trained model to local model directory\u001b[39;49;00m\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mSaving trained MLM to [/tmp/cache/model/finetuned/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        trainer.save_model(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/finetuned\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m os.path.exists(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/finetuned/pytorch_model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m os.path.exists(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/finetuned/config.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "            \u001b[37m# Copy trained model from local directory of the training cluster to S3 \u001b[39;49;00m\n",
      "            logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCopying saved model from local to [s3://\u001b[39;49;00m\u001b[33m{S3_BUCKET}\u001b[39;49;00m\u001b[33m/model/finetuned/]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            s3.meta.client.upload_file(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/finetuned/pytorch_model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, S3_BUCKET, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel/finetuned/pytorch_model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            s3.meta.client.upload_file(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/finetuned/config.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, S3_BUCKET, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel/finetuned/config.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "            \u001b[37m# Copy vocab.txt to local model directory - this is needed to re-create the trained MLM\u001b[39;49;00m\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mCopying BERT vocabulary to local model artifacts location (EBS) to faciliate model evaluation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            shutil.copyfile(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{args.input_dir}\u001b[39;49;00m\u001b[33m/vocab/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/finetuned/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "            \u001b[37m# Copy vocab.txt to saved model artifacts location in S3\u001b[39;49;00m\n",
      "            logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCopying custom vocabulary from [\u001b[39;49;00m\u001b[33m{path}\u001b[39;49;00m\u001b[33m/vocab.txt] to [s3://\u001b[39;49;00m\u001b[33m{S3_BUCKET}\u001b[39;49;00m\u001b[33m/model/finetuned/] for future stages of ML pipeline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            s3.meta.client.upload_file(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{path}\u001b[39;49;00m\u001b[33m/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, S3_BUCKET, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel/finetuned/vocab.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "            \u001b[37m# Evaluate the trained model \u001b[39;49;00m\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mCreate fill-mask task pipeline to evaluate trained MLM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            fill_mask = pipeline(\u001b[33m'\u001b[39;49;00m\u001b[33mfill-mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, model=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/cache/model/finetuned\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            df = pd.read_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{S3_BUCKET}\u001b[39;49;00m\u001b[33m/data/eval/eval_mlm.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "            \u001b[34mfor\u001b[39;49;00m gt, masked_sentence \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(df.ground_truth.tolist(), df.masked.tolist()):\n",
      "                logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mGround Truth    : \u001b[39;49;00m\u001b[33m{gt}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mMasked sentence : \u001b[39;49;00m\u001b[33m{masked_sentence}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                predictions = fill_mask(masked_sentence, top_k=\u001b[34m3\u001b[39;49;00m)\n",
      "                \u001b[34mfor\u001b[39;49;00m i, prediction \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(predictions):\n",
      "                    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mRank: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mi+1} | \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33m(prediction[\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m] * 100):.2f} \u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m | \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33m[prediction[\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtoken_str\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m]]}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m * \u001b[34m10\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./src/fine_tune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97833e7d-8f16-440e-af8f-8ba04924f446",
   "metadata": {},
   "source": [
    "#### Create the estimator \n",
    "\n",
    "* Documentation on SageMaker HuggingFace Estimator can be found [here](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b497a195-b529-4b04-8700-fc8cb4442e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {'train': f's3://{S3_BUCKET}/data'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71388f75-b743-4253-b276-bb462db96629",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512  # Context size for BERT tokenizer \n",
    "CHUNK_SIZE = 128  \n",
    "TRAIN_EPOCHS = 40\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa57c97-ae44-47a6-8c60-2fe02c907813",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {'s3_bucket': S3_BUCKET, \n",
    "                   'max_len': MAX_LENGTH,\n",
    "                   'chunk_size': CHUNK_SIZE,\n",
    "                   'num_train_epochs': TRAIN_EPOCHS, \n",
    "                   'per_device_train_batch_size': BATCH_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "663fe3a4-8ee9-44ed-82de-26a92ed62535",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTRIBUTION_STRATEGY = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eefb754-36dc-4422-bcfc-bf80270ca09f",
   "metadata": {},
   "source": [
    "#### Create HuggingFace estimator needed for the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66f088c5-7557-4cc8-973a-0fb8195b3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point=ENTRY_POINT, \n",
    "                                    source_dir=SOURCE_DIR, \n",
    "                                    role=ROLE, \n",
    "                                    instance_type=INSTANCE_TYPE, \n",
    "                                    instance_count=INSTANCE_COUNT,\n",
    "                                    volume_size=EBS_VOLUME_SIZE,\n",
    "                                    hyperparameters=HYPERPARAMETERS,\n",
    "                                    distribution=DISTRIBUTION_STRATEGY,\n",
    "                                    transformers_version=TRANSFORMERS_VERSION, \n",
    "                                    pytorch_version=PYTORCH_VERSION, \n",
    "                                    py_version=PYTHON_VERSION, \n",
    "                                    disable_profiler=True,\n",
    "                                    debugger_hook_config=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ad26c-96c8-4a48-ad36-e32a600e9598",
   "metadata": {},
   "source": [
    "#### Kick-off the fine-tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "899bce1e-db21-4f21-921c-5cc0ac09cea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating training-job with name: huggingface-pytorch-training-2022-08-26-23-22-45-230\n",
      "train request: {\n",
      "    \"AlgorithmSpecification\": {\n",
      "        \"TrainingInputMode\": \"File\",\n",
      "        \"TrainingImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\",\n",
      "        \"EnableSageMakerMetricsTimeSeries\": true\n",
      "    },\n",
      "    \"OutputDataConfig\": {\n",
      "        \"S3OutputPath\": \"s3://sagemaker-us-east-1-119174016168/\"\n",
      "    },\n",
      "    \"TrainingJobName\": \"huggingface-pytorch-training-2022-08-26-23-22-45-230\",\n",
      "    \"StoppingCondition\": {\n",
      "        \"MaxRuntimeInSeconds\": 86400\n",
      "    },\n",
      "    \"ResourceConfig\": {\n",
      "        \"VolumeSizeInGB\": 1024,\n",
      "        \"InstanceCount\": 4,\n",
      "        \"InstanceType\": \"ml.p4d.24xlarge\"\n",
      "    },\n",
      "    \"RoleArn\": \"arn:aws:iam::119174016168:role/service-role/AmazonSageMaker-ExecutionRole-20211014T093628\",\n",
      "    \"InputDataConfig\": [\n",
      "        {\n",
      "            \"DataSource\": {\n",
      "                \"S3DataSource\": {\n",
      "                    \"S3DataType\": \"S3Prefix\",\n",
      "                    \"S3Uri\": \"s3://sagemaker-us-east-1-119174016168/data\",\n",
      "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
      "                }\n",
      "            },\n",
      "            \"ChannelName\": \"train\"\n",
      "        }\n",
      "    ],\n",
      "    \"HyperParameters\": {\n",
      "        \"s3_bucket\": \"\\\"sagemaker-us-east-1-119174016168\\\"\",\n",
      "        \"max_len\": \"512\",\n",
      "        \"chunk_size\": \"128\",\n",
      "        \"num_train_epochs\": \"40\",\n",
      "        \"per_device_train_batch_size\": \"32\",\n",
      "        \"sagemaker_submit_directory\": \"\\\"s3://sagemaker-us-east-1-119174016168/huggingface-pytorch-training-2022-08-26-23-22-45-230/source/sourcedir.tar.gz\\\"\",\n",
      "        \"sagemaker_program\": \"\\\"fine_tune.py\\\"\",\n",
      "        \"sagemaker_container_log_level\": \"20\",\n",
      "        \"sagemaker_job_name\": \"\\\"huggingface-pytorch-training-2022-08-26-23-22-45-230\\\"\",\n",
      "        \"sagemaker_region\": \"\\\"us-east-1\\\"\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": \"true\",\n",
      "        \"sagemaker_instance_type\": \"\\\"ml.p4d.24xlarge\\\"\",\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\\\"\\\"\"\n",
      "    },\n",
      "    \"Environment\": {\n",
      "        \"USE_SMDEBUG\": \"0\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(DATA, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc27a2b-4007-47cf-9a98-d2cf2fa81353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d0f36-5666-4b51-8af3-224d5b6bf8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66dfaa-cdf4-4389-8243-77fcdc07d2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
